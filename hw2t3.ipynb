{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90d7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b381fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5d2d185d-ecce-4e63-93ac-5841f01b6032;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 287ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5d2d185d-ecce-4e63-93ac-5841f01b6032\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/14 21:01:14 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/14 21:01:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/14 21:01:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/14 21:01:14 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/14 21:01:16 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.14.0.jar added multiple times to distributed cache.\n",
      "22/04/14 21:01:16 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.8.0.jar added multiple times to distributed cache.\n",
      "22/04/14 21:01:16 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-2.3.4.jar added multiple times to distributed cache.\n",
      "22/04/14 21:01:16 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar added multiple times to distributed cache.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.option(\"delimiter\", \"\\t\").csv('hdfs://hw2-m/user/root/small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353f29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 21:01:37 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1649870565436_0013_01_000001 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:37.418]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000001\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:01:37 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1649870565436_0013_01_000001 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:37.418]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000001\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:01:37 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on hw2-w-1.c.csee4121-339916.internal: Container from a bad node: container_1649870565436_0013_01_000001 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:37.418]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000001\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:01:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 3.0 (TID 5) (hw2-w-1.c.csee4121-339916.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1649870565436_0013_01_000001 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:37.418]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000001\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:37.421]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10252 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 1 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000001/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000001/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_count = df.groupBy(\"_c0\").count()\n",
    "newdf = df_count.withColumn(\"rank\", lit(1))\n",
    "newdf = newdf.rdd.map(lambda x: (x[0], x[1], x[2], x[2]/x[1])).toDF(['article', 'count', 'rank', 'contribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beeb8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 21:01:53 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1649870565436_0013_01_000002 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:53.285]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000002\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:53.288]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:53.289]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:01:53 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1649870565436_0013_01_000002 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:53.285]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000002\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:53.288]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:53.289]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:01:53 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on hw2-w-0.c.csee4121-339916.internal: Container from a bad node: container_1649870565436_0013_01_000002 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:01:53.285]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000002\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:01:53.288]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:01:53.289]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12289 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 2 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000002/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000002/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 21:02:29 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1649870565436_0013_01_000005 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:02:29.592]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000005\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:02:29 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1649870565436_0013_01_000005 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:02:29.592]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000005\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:02:29 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on hw2-w-0.c.csee4121-339916.internal: Container from a bad node: container_1649870565436_0013_01_000005 on host: hw2-w-0.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:02:29.592]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000005\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:02:29.594]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 12364 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 4 --hostname hw2-w-0.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000005/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000005/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 21:03:06 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1649870565436_0013_01_000004 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:03:06.279]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000004\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:03:06.282]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:03:06.283]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:03:06 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1649870565436_0013_01_000004 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:03:06.279]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000004\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:03:06.282]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:03:06.283]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n",
      "22/04/14 21:03:06 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 3 on hw2-w-1.c.csee4121-339916.internal: Container from a bad node: container_1649870565436_0013_01_000004 on host: hw2-w-1.c.csee4121-339916.internal. Exit status: 134. Diagnostics: [2022-04-14 21:03:06.279]Exception from container-launch.\n",
      "Container id: container_1649870565436_0013_01_000004\n",
      "Exit code: 134\n",
      "\n",
      "[2022-04-14 21:03:06.282]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      "[2022-04-14 21:03:06.283]Container exited with a non-zero exit code 134. Error file: prelaunch.err.\n",
      "Last 4096 bytes of prelaunch.err :\n",
      "/bin/bash: line 1: 10329 Aborted                 /usr/lib/jvm/temurin-8-jdk-amd64/bin/java -server -Xmx5739m -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/tmp '-Dspark.driver.port=46151' '-Dspark.ui.port=0' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@hw2-m.c.csee4121-339916.internal:46151 --executor-id 3 --hostname hw2-w-1.c.csee4121-339916.internal --cores 2 --app-id application_1649870565436_0013 --resourceProfileId 0 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/com.databricks_spark-xml_2.12-0.14.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/commons-io_commons-io-2.8.0.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.glassfish.jaxb_txw2-2.3.4.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1649870565436_0013/container_1649870565436_0013_01_000004/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar > /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stdout 2> /var/log/hadoop-yarn/userlogs/application_1649870565436_0013/container_1649870565436_0013_01_000004/stderr\n",
      "Last 4096 bytes of stderr :\n",
      "\n",
      "\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    newdf = df.join(newdf, df._c1 == newdf.article, 'inner')\n",
    "    newdf = newdf.groupBy(['_c0', 'count']).sum('contribution')\n",
    "    newdf = newdf.rdd.map(lambda x: (x[0], x[1], 0.15 + 0.85 * x[2], \n",
    "                                     (0.15 + 0.85 * x[2])/x[1])).toDF(['article', 'count', 'rank', 'contribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63e094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 393:========================>                                (3 + 4) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|          article|               rank|\n",
      "+-----------------+-------------------+\n",
      "|              833|0.15006396909414746|\n",
      "|     Anbar (town)|0.15006396909414746|\n",
      "|Abbasid Caliphate|0.15006396909414746|\n",
      "|              743|0.15006396909414746|\n",
      "|              759|0.15006396909414746|\n",
      "|              768|0.15006396909414746|\n",
      "|              701|0.15006396909414746|\n",
      "|             1073|0.15006396909414746|\n",
      "|              847|0.15006396909414746|\n",
      "|              909|0.15006396909414746|\n",
      "|              968|0.15006396909414746|\n",
      "|  Seventh Century|0.15006396909414746|\n",
      "|              842|0.15006396909414746|\n",
      "|              650|0.15006396909414746|\n",
      "|              751|0.15006396909414746|\n",
      "|              785|0.15006396909414746|\n",
      "|              931|0.15006396909414746|\n",
      "|              850|0.15006396909414746|\n",
      "|              684|0.15006396909414746|\n",
      "|           Uzbeks|0.15006396909414746|\n",
      "+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 393:========================================>                (5 + 2) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newdf.sort(newdf.rank.asc()).select('article', 'rank').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2945d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newdf.sort(newdf.rank.asc()).select('article', 'rank').repartition(10).write.option(\"delimiter\", \"\\t\").csv('task3_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3256ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newdf.sort(newdf.rank.asc()).select('article', 'rank').limit(5).toPandas().to_csv('/home/jz3313/hw2/p1t3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
